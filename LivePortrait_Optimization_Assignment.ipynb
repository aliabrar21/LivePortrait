{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup & Clone Repo\n",
    "!git clone https://github.com/facebookresearch/LivePortrait.git\n",
    "%cd LivePortrait\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "!pip install torch torchvision --quiet\n",
    "\n",
    "print(\"Setup done ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Libraries and Load Model\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace below with actual LivePortrait model import and load\n",
    "class LivePortraitModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Init model layers here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass here\n",
    "        return x  # dummy pass through\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LivePortraitModel()\n",
    "# model.load_state_dict(torch.load('path_to_weights.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Model loaded ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61fb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Preprocessing and Postprocessing\n",
    "def preprocess(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize((256,256))  # adjust size if needed\n",
    "    image = torch.tensor([[[pixel/255 for pixel in channel] for channel in image.split()]]).float()\n",
    "    return image.to(device)\n",
    "\n",
    "def postprocess(output_tensor):\n",
    "    output_tensor = output_tensor.squeeze().cpu().clamp(0,1)\n",
    "    output_np = output_tensor.permute(1,2,0).numpy()\n",
    "    return Image.fromarray((output_np * 255).astype('uint8'))\n",
    "\n",
    "print(\"Preprocessing ready ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbeb2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    #@title Original Inference & Timing
import time
import matplotlib.pyplot as plt
from PIL import Image
import requests
from io import BytesIO
import torchvision.transforms as transforms

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def preprocess_from_url(image_url):
    response = requests.get(image_url)
    image = Image.open(BytesIO(response.content)).convert('RGB')
    image = image.resize((256, 256))  # adjust size if needed

    transform = transforms.Compose([
        transforms.ToTensor(),
    ])
    tensor = transform(image).unsqueeze(0)
    return tensor.to(device)

input_image_url = 'https://www.weareteachers.com/wp-content/uploads/inferences-anchor-charts-simple.jpg'
input_tensor = preprocess_from_url(input_image_url)

start_time = time.time()
with torch.no_grad():
    output = model(input_tensor)
end_time = time.time()

original_time = end_time - start_time
print(f"Original inference time: {original_time:.4f} seconds")

def postprocess(output_tensor):
    output_tensor = output_tensor.squeeze().cpu().clamp(0, 1)
    output_np = output_tensor.permute(1, 2, 0).numpy()
    return Image.fromarray((output_np * 255).astype('uint8'))

plt.imshow(postprocess(output))
plt.title("Original Output")
plt.axis('off')
plt.show()
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optimized Inference with Mixed Precision\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    with autocast():\n",
    "        output_opt = model(input_tensor)\n",
    "end_time = time.time()\n",
    "\n",
    "optimized_time = end_time - start_time\n",
    "print(f\"Optimized inference time (mixed precision): {optimized_time:.4f} seconds\")\n",
    "\n",
    "plt.imshow(postprocess(output_opt))\n",
    "plt.title(\"Optimized Output (Mixed Precision)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d371ff",
   "metadata": {},
   "source": [
    "# Summary of Optimizations\n",
    "\n",
    "- Used mixed precision inference (`torch.cuda.amp`) to speed up inference and reduce memory.\n",
    "- Ensured model eval mode and no-grad context.\n",
    "- Kept data on GPU to avoid transfer overhead.\n",
    "\n",
    "# Performance Comparison\n",
    "\n",
    "| Metric               | Original        | Optimized (Mixed Precision) |\n",
    "|----------------------|-----------------|-----------------------------|\n",
    "| Inference Time (sec)  | original_time   | optimized_time              |\n",
    "| GPU Memory Usage      | Baseline        | Reduced                    |\n",
    "| Output Quality       | Baseline        | Comparable (visually similar)|\n",
    "\n",
    "# Future Optimizations\n",
    "\n",
    "- Convert to TorchScript or ONNX for faster inference.\n",
    "- Model pruning or quantization.\n",
    "- Layer fusion or CUDA kernel tuning.\n",
    "- Batch size tuning for throughput.\n",
    "\n",
    "---\n",
    "\n",
    "Replace placeholders with your actual model, weights, and image paths. Run all cells on GPU in Colab and fill in timings/output quality.\n",
    "\n",
    "If you want me to generate a downloadable `.ipynb` notebook file or help with exact repo code integration, just say the word!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
